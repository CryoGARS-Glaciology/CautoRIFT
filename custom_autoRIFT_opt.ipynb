{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb75ff29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Open Source version of ISCE.\n",
      "Some of the workflows depend on a separate licensed package.\n",
      "To obtain the licensed package, please make a request for ISCE\n",
      "through the website: https://download.jpl.nasa.gov/ops/request/index.cfm.\n",
      "Alternatively, if you are a member, or can become a member of WinSAR\n",
      "you may be able to obtain access to a version of the licensed sofware at\n",
      "https://winsar.unavco.org/software/isce\n"
     ]
    }
   ],
   "source": [
    "import rasterio as rio\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from scipy.interpolate import interp2d\n",
    "import pandas as pd\n",
    "\n",
    "from geogrid import GeogridOptical\n",
    "from autoRIFT import autoRIFT\n",
    "from osgeo import gdal, osr\n",
    "import struct\n",
    "import re\n",
    "from datetime import date\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib.request\n",
    "import time\n",
    "import subprocess\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "\n",
    "from autorift_utilities import rio_write\n",
    "\n",
    "gdal.AllRegister() # register all GDAL drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb97a11",
   "metadata": {},
   "source": [
    "## 1) Resample DEM and other autoRIFT/geogrid input rasters to the desired chip size\n",
    "\n",
    "Recommended chip size is >= 16*pixel_resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee59d3",
   "metadata": {},
   "source": [
    "#### Customizable parameters for geogrid:\n",
    "\n",
    "    dhdx, dhdy:              x/y local surface slope maps (unitless)\n",
    "    vx,vy:                   x/y reference velocity maps (in units of m/yr)\n",
    "    srx, sry:                x/y velocity search range limit maps (in units of m/yr)\n",
    "    csminx, csminy:          x/y chip size minimum maps (in units of m; constant ratio between x and y)\n",
    "    csmaxx, csmaxy:          x/y chip size maximum maps (in units of m; constant ratio between x and y)\n",
    "    ssm:                     stable surface mask (boolean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447cdbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ENTER CHIP SIZE, DEM INFO, AND REFERENCE VELOCITY INFO ########### \n",
    "CHIPSIZE_M = 200 # enter in desired grid size in meters (default is 32 pixels)\n",
    "\n",
    "# enter in the path to your best DEM over the region\n",
    "# dempath = '/Users/jukesliu/Documents/TURNER/DATA/ICE_THICKNESS/surface/DEMs_previous/'\n",
    "dempath = '/Users/jukesliu/Documents/TURNER/DATA/ICE_THICKNESS/Variegated/'\n",
    "demname = 'Variegated_20070903_clipped.tif'\n",
    "\n",
    "# path to the reference files for geogrid (vx, vy, ssm)\n",
    "refvpath = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/forAutoRIFT/' \n",
    "# vx_fname = 'vx_cropped.tif' # name of reference vx file\n",
    "# vy_fname = 'vy_cropped.tif' # name of reference vy file\n",
    "# refvpath = '/Volumes/SGlacier/' \n",
    "vx_fname = '' # name of reference vx file\n",
    "vy_fname = '' # name of reference vy file\n",
    "ssm_name = 'ice_mask_200mbuffer.tif' # name of the stable surface mask (all off-ice areas)\n",
    "\n",
    "sr_scaling = 16 # multiply by vx and vy to generate search range limits\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53dfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and grab info from your DEM\n",
    "refdem = rio.open(dempath+demname) # open DEM using rasterio\n",
    "elev = refdem.read(1) # read in the first and only band (elevations)\n",
    "\n",
    "# grab the x and y grid values from the DEM:\n",
    "dem_x = np.linspace(refdem.bounds.left, refdem.bounds.right, num=np.shape(elev)[1])\n",
    "dem_y = np.linspace(refdem.bounds.top, refdem.bounds.bottom, num=np.shape(elev)[0])\n",
    "\n",
    "# grab the resampled x and y grid values from the DEM\n",
    "new_x = np.arange(refdem.bounds.left, refdem.bounds.right, CHIPSIZE_M)\n",
    "new_y = np.arange(refdem.bounds.top, refdem.bounds.bottom, -CHIPSIZE_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641ca8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variegated_20070903_clipped_200m.tif  already exists.\n"
     ]
    }
   ],
   "source": [
    "# Resample the DEM to the input chip size\n",
    "dem_outfile = demname[:-4]+'_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "\n",
    "if not os.path.exists(dempath+dem_outfile): # if the resampled DEM does not exist already\n",
    "    # Create thew new x and y grid values using DEM bounds and the chipsize\n",
    "    dem_resamp = np.zeros((len(new_y), len(new_x))) # create an empty resampled DEM grid\n",
    "    print(dem_resamp.shape)\n",
    "    \n",
    "    # Resample to your new DEM bounds\n",
    "    f = interp2d(dem_x, dem_y, elev) # create DEM interpolation object\n",
    "    dem_resamp = f(new_x,new_y) # resample the NIR data to the DSM coordinates\n",
    "    dem_resamp = np.flipud(dem_resamp) # flip up down\n",
    "    print(\"Resampled to new dimensions:\",dem_resamp.shape)\n",
    "    \n",
    "    # Display the two DEMs as a visual check\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    im1 = ax1.imshow(elev, cmap='Greys_r', vmin=0)\n",
    "    ax1.set_title('Original DEM: '+str(refdem.transform[0])+' m') # original spatial resolution\n",
    "    fig.colorbar(im1, ax=ax1,label='Elevation [m]')\n",
    "\n",
    "    im2 = ax2.imshow(dem_resamp, cmap='Greys_r', vmin=0)\n",
    "    ax2.set_title('Resampled DEM: '+str(CHIPSIZE_M)+' m') # new spatial resolution\n",
    "    fig.colorbar(im2, ax=ax2,label='Elevation [m]')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the resampled DEM to georeferenced tif file\n",
    "    print(\"Save resampled DEM to\", dempath+dem_outfile)\n",
    "    rio_write(dempath+dem_outfile, dem_resamp, refdem, CHIPSIZE_M)\n",
    "else:\n",
    "    # load the empty grid\n",
    "    dem_resamp = np.zeros((len(new_y), len(new_x))) # create an empty resampled DEM grid\n",
    "    print(dem_outfile, ' already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d21bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically set file names\n",
    "dhdx_outfile = demname[:-4]+'_'+str(CHIPSIZE_M)+'m_dhdx.tif' # generate new filename\n",
    "dhdy_outfile = demname[:-4]+'_'+str(CHIPSIZE_M)+'m_dhdy.tif' # generate new filename\n",
    "ssm_outfile = 'ssm_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "vx_outfile = 'vx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "vy_outfile = 'vy_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "srx_outfile = 'srx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "sry_outfile = 'sry_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "csminx_fname = 'csminx_'+str(CHIPSIZE_M)+'m.tif'\n",
    "csminy_fname = 'csminy_'+str(CHIPSIZE_M)+'m.tif'\n",
    "csmaxx_fname = 'csmaxx_'+str(CHIPSIZE_M)+'m.tif'\n",
    "csmaxy_fname = 'csmaxy_'+str(CHIPSIZE_M)+'m.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d7bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_geogrid_inputs(CHIPSIZE_M, dempath, demname, refvpath, vx_fname, vy_fname, sr_scaling):\n",
    "    import rasterio as rio\n",
    "    import os\n",
    "    \n",
    "    # GRAB DEM INFO\n",
    "    refdem = rio.open(dempath+demname) # open DEM using rasterio\n",
    "    elev = refdem.read(1) # read in the first and only band (elevations)\n",
    "\n",
    "    # grab the x and y grid values from the DEM:\n",
    "    dem_x = np.linspace(refdem.bounds.left, refdem.bounds.right, num=np.shape(elev)[1])\n",
    "    dem_y = np.linspace(refdem.bounds.top, refdem.bounds.bottom, num=np.shape(elev)[0])\n",
    "\n",
    "    # grab the resampled x and y grid values from the DEM\n",
    "    new_x = np.arange(refdem.bounds.left, refdem.bounds.right, CHIPSIZE_M)\n",
    "    new_y = np.arange(refdem.bounds.top, refdem.bounds.bottom, -CHIPSIZE_M)\n",
    "    \n",
    "    # RESAMPLE THE DEM\n",
    "    dem_outfile = demname[:-4]+'_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    if not os.path.exists(dempath+dem_outfile): # if the resampled DEM does not exist already\n",
    "        # Create thew new x and y grid values using DEM bounds and the chipsize\n",
    "        dem_resamp = np.zeros((len(new_y), len(new_x))) # create an empty resampled DEM grid\n",
    "        print(dem_resamp.shape)\n",
    "\n",
    "        # Resample to your new DEM bounds\n",
    "        f = interp2d(dem_x, dem_y, elev) # create DEM interpolation object\n",
    "        dem_resamp = f(new_x,new_y) # resample the NIR data to the DSM coordinates\n",
    "        dem_resamp = np.flipud(dem_resamp) # flip up down\n",
    "        print(\"Resampled to new dimensions:\",dem_resamp.shape)\n",
    "\n",
    "        # Display the two DEMs as a visual check\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        im1 = ax1.imshow(elev, cmap='Greys_r', vmin=0)\n",
    "        ax1.set_title('Original DEM: '+str(refdem.transform[0])+' m') # original spatial resolution\n",
    "        fig.colorbar(im1, ax=ax1,label='Elevation [m]')\n",
    "\n",
    "        im2 = ax2.imshow(dem_resamp, cmap='Greys_r', vmin=0)\n",
    "        ax2.set_title('Resampled DEM: '+str(CHIPSIZE_M)+' m') # new spatial resolution\n",
    "        fig.colorbar(im2, ax=ax2,label='Elevation [m]')\n",
    "        plt.show()\n",
    "\n",
    "        # Save the resampled DEM to georeferenced tif file\n",
    "        print(\"Save resampled DEM to\", dempath+dem_outfile)\n",
    "        rio_write(dempath+dem_outfile, dem_resamp, refdem, CHIPSIZE_M)\n",
    "    else:\n",
    "        # load the existing resampled DEM\n",
    "        dem_r = rio.open(dempath+dem_outfile) # open DEM using rasterio\n",
    "        dem_resamp = dem_r.read(1) # read in the first and only band (elevations)\n",
    "        print(dem_outfile, ' already exists.')\n",
    "    \n",
    "    # CREATE DHDX, DHDY\n",
    "    dhdx_outfile = demname[:-4]+'_'+str(CHIPSIZE_M)+'m_dhdx.tif' # generate new filename\n",
    "    dhdy_outfile = demname[:-4]+'_'+str(CHIPSIZE_M)+'m_dhdy.tif' # generate new filename\n",
    "    if not os.path.exists(dempath+dhdx_outfile) or not os.path.exists(dempath+dhdy_outfile): # if either is missing\n",
    "        # Produce dhdx and dhdy maps from resampled DEM\n",
    "        dhdx = np.gradient(dem_resamp, axis=1)/CHIPSIZE_M\n",
    "        dhdy = np.gradient(dem_resamp, axis=0)/CHIPSIZE_M\n",
    "\n",
    "        # Filter out borders with high gradient values\n",
    "        grad_thresh = 5\n",
    "        dhdx[abs(dhdx) > grad_thresh] = 0; dhdy[abs(dhdy) > grad_thresh] = 0\n",
    "\n",
    "        # absolute value of the max gradient values expected:\n",
    "        dhmax = 1\n",
    "\n",
    "        # Display the two DEMs as a visual check\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "        im1 = ax1.imshow(dhdx, cmap='Greys_r', vmin=-dhmax, vmax=dhmax)\n",
    "        ax1.set_title('dhdx') # surface slope x\n",
    "        fig.colorbar(im1, ax=ax1)\n",
    "\n",
    "        im2 = ax2.imshow(dhdy, cmap='Greys_r', vmin=-dhmax, vmax=dhmax)\n",
    "        ax2.set_title('dhdy') # surface slope y\n",
    "        fig.colorbar(im2, ax=ax2)\n",
    "        plt.show()\n",
    "\n",
    "        # Save the gradient maps to tif files\n",
    "        print(\"Save surface slope maps to\", dempath)\n",
    "        rio_write(dempath+dhdx_outfile, dhdx, refdem, CHIPSIZE_M) # dhdx\n",
    "        rio_write(dempath+dhdy_outfile, dhdy, refdem, CHIPSIZE_M)\n",
    "    else:\n",
    "        print(dhdy_outfile, 'and', dhdx_outfile, 'already exist.')\n",
    "\n",
    "    # VX, VY, SRX, SRY\n",
    "    vx_outfile = 'vx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    vy_outfile = 'vy_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    srx_outfile = 'srx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    sry_outfile = 'sry_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "    if vx_fname != '':\n",
    "        if not os.path.exists(refvpath+vx_outfile) or not os.path.exists(refvpath+vy_outfile): # if either vx, vy missing\n",
    "            # open the files with rasterio\n",
    "            vx_reader = rio.open(refvpath+vx_fname); vx0 = vx_reader.read(1)\n",
    "            vy_reader = rio.open(refvpath+vy_fname); vy0 = vy_reader.read(1)\n",
    "            vx_x = np.linspace(vx_reader.bounds.left, vx_reader.bounds.right, num=np.shape(vx0)[1])\n",
    "            vx_y = np.linspace(vx_reader.bounds.top, vx_reader.bounds.bottom, num=np.shape(vx0)[0])\n",
    "            vy_x = np.linspace(vy_reader.bounds.left, vy_reader.bounds.right, num=np.shape(vy0)[1])\n",
    "            vy_y = np.linspace(vy_reader.bounds.top, vy_reader.bounds.bottom, num=np.shape(vy0)[0])\n",
    "\n",
    "            # Resample to the DEM grid\n",
    "            fx = interp2d(vx_x, vx_y, vx0)\n",
    "            fy = interp2d(vy_x, vy_y, vy0)\n",
    "            vx_resamp = np.flipud(fx(new_x,new_y)) \n",
    "            vy_resamp = np.flipud(fy(new_x,new_y)) # flip up down\n",
    "\n",
    "            # Display the two velocity files as a visual check\n",
    "            fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "            im1 = ax1.imshow(vx_resamp, cmap='Greys_r'); ax1.set_title('vx'); fig.colorbar(im1, ax=ax1)\n",
    "            im2 = ax2.imshow(vy_resamp, cmap='Greys_r'); ax2.set_title('vy'); fig.colorbar(im2, ax=ax2)\n",
    "            plt.show()\n",
    "\n",
    "            # CALCULATE SEARCH RANGE LIMITS MULTIPLY VX AND VY BY SOME NUMBER\n",
    "            srx_resamp = vx_resamp*sr_scaling; sry_resamp = vy_resamp*sr_scaling\n",
    "\n",
    "            # Display the two search range files as a visual check\n",
    "            fig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "            im1 = ax1.imshow(srx_resamp, cmap='Greys_r'); ax1.set_title('srx'); fig.colorbar(im1, ax=ax1)\n",
    "            im2 = ax2.imshow(sry_resamp, cmap='Greys_r'); ax2.set_title('sry'); fig.colorbar(im2, ax=ax2)\n",
    "            plt.show()\n",
    "\n",
    "            # save the reference velocity and search range maps\n",
    "            rio_write(refvpath+vx_outfile, vx_resamp, refdem, CHIPSIZE_M) # vx\n",
    "            rio_write(refvpath+vy_outfile, vy_resamp, refdem, CHIPSIZE_M) # vy\n",
    "            rio_write(refvpath+srx_outfile, srx_resamp, refdem, CHIPSIZE_M) # srx\n",
    "            rio_write(refvpath+sry_outfile, sry_resamp, refdem, CHIPSIZE_M) # sry\n",
    "        else:\n",
    "            print(vx_outfile, ',', vy_outfile, ',', srx_outfile, ',', sry_outfile, 'already exist.')  \n",
    "    \n",
    "    # MASKS\n",
    "    ssm_outfile = 'ssm_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "\n",
    "    if not os.path.exists(refvpath+ssm_outfile):  # overwrite all\n",
    "        if os.path.exists(refvpath+ssm_name):\n",
    "            # read it in, process (resample, mask, etc.) and resave\n",
    "            ssmreader = rio.open(refvpath+ssm_name)\n",
    "            ssm = ssmreader.read(1)\n",
    "            ssm[ssm > 0] = 1; #ssm[ssm < 0.0] = 0; # make binary\n",
    "            ssm = ssm < 1 # find all stable areas (where.tif = 0)\n",
    "\n",
    "            # grab x and y-values\n",
    "            ssm_x = np.linspace(ssmreader.bounds.left, ssmreader.bounds.right, num=np.shape(ssm)[1])\n",
    "            ssm_y = np.linspace(ssmreader.bounds.top, ssmreader.bounds.bottom, num=np.shape(ssm)[0])\n",
    "\n",
    "            # Resample to the DEM grid\n",
    "            f_ssm = interp2d(ssm_x, ssm_y, ssm)\n",
    "\n",
    "            ssm_resamp = np.flipud(f_ssm(new_x,new_y))\n",
    "\n",
    "            # plot\n",
    "            fig, ax = plt.subplots(1,1)\n",
    "            ssm_im = ax.imshow(ssm_resamp,cmap='gray',vmin=0)\n",
    "            ax.set_title('Stable Surface Mask')\n",
    "            fig.colorbar(ssm_im, ax=ax)\n",
    "            plt.show()\n",
    "\n",
    "            # export\n",
    "            rio_write(refvpath+ssm_outfile, ssm_resamp, refdem, CHIPSIZE_M)\n",
    "    else:\n",
    "        print(ssm_outfile,'already exists.')\n",
    "    \n",
    "    return dem_outfile, dhdx_outfile, dhdy_outfile, vx_outfile, vy_outfile, srx_outfile, sry_outfile, ssm_outfile\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8252e3e",
   "metadata": {},
   "source": [
    "## 2) Function to geogrid with resampled DEM and other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8410bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_geogrid_inhouse(out_path, img_type, indir_m, indir_s, MINCHIPSIZE, NO_DATA_VAL, dem, # required inputs\n",
    "                        dhdx, dhdy, vx, vy, srx, sry, csminx, csminy, csmaxx, csmaxy, ssm, # optional inputs\n",
    "                       ):\n",
    "    import rasterio as rio\n",
    "    import os\n",
    "    \n",
    "    CHIPSIZE_M = MINCHIPSIZE # set minimum chip size equal\n",
    "    ############ Clear all old geogrid files ##########################\n",
    "    for file in os.listdir(out_path):\n",
    "        if file.startswith('window') and file.endswith('.tif'):\n",
    "            print('removed', file)\n",
    "            os.remove(out_path+file)\n",
    "    print('Old files cleared.'); print()\n",
    "\n",
    "    dem_info = gdal.Info(dem, format='json') # grab info from DEM\n",
    "    print('Obtained DEM info.'); print()\n",
    "#     print(dem_info)\n",
    "#     print(dem_info['geoTransform'][1])\n",
    "\n",
    "    ############ Run geogrid optical ##########################\n",
    "    if img_type == 'OPT': # Optical images\n",
    "        print('Processing optical images with geogrid.'); print()\n",
    "        obj = GeogridOptical() # initialize geogrid object\n",
    "\n",
    "        ############ Coregister the optical data (from coregisterLoadMetadataOptical) #############\n",
    "        x1a, y1a, xsize1, ysize1, x2a, y2a, xsize2, ysize2, trans = obj.coregister(indir_m, indir_s)\n",
    "#         print(trans)\n",
    "\n",
    "        # grab dates from file names\n",
    "        im1_name = indir_m.split('/')[-1]; im2_name = indir_s.split('/')[-1]\n",
    "        if 'LC' in im1_name and 'LC' in im2_name:\n",
    "            ds1 = im1_name.split('_')[3]\n",
    "            ds2 = im2_name.split('_')[3]\n",
    "        elif 'S2' in im1_name and 'S2' in im2_name:\n",
    "            ds1 = im1_name.split('_')[2]\n",
    "            ds2 = im2_name.split('_')[2]\n",
    "        elif 'PS' in im1_name and 'PS' in im2_name:\n",
    "            ds1 = im1_name.split('_')[1]\n",
    "            ds2 = im2_name.split('_')[1]\n",
    "        else:\n",
    "            raise Exception('Optical data NOT supported yet!') \n",
    "        print('Optical images coregistered.'); print()\n",
    "\n",
    "        ########### Load geogrid inputs and run (from runGeogridOptical) ################\n",
    "\n",
    "        # grab info from above\n",
    "        obj.startingX = trans[0]; obj.startingY = trans[3]\n",
    "        obj.XSize = trans[1]; obj.YSize = trans[5]\n",
    "        d0 = datetime.date(int(ds1[0:4]),int(ds1[4:6]),int(ds1[6:8]))\n",
    "        d1 = datetime.date(int(ds2[0:4]),int(ds2[4:6]),int(ds2[6:8]))\n",
    "        date_dt_base = d1 - d0\n",
    "        obj.repeatTime = date_dt_base.total_seconds()\n",
    "        obj.numberOfLines = ysize1; obj.numberOfSamples = xsize1\n",
    "        obj.gridSpacingX = dem_info['geoTransform'][1] # output grid spacing is the same as the DEM\n",
    "\n",
    "        # customize no data value and minimimum chip size\n",
    "        obj.nodata_out = NO_DATA_VAL\n",
    "        obj.chipSizeX0 = MINCHIPSIZE\n",
    "\n",
    "        # set raster paths and names\n",
    "        obj.dat1name = indir_m # first image\n",
    "        obj.demname = dem # DEM\n",
    "        obj.dhdxname = dhdx; obj.dhdyname = dhdy # surface slope\n",
    "        obj.vxname = vx; obj.vyname = vy # reference velocity\n",
    "        obj.srxname = srx; obj.sryname = sry # search range limits\n",
    "        obj.csminxname = csminx; obj.csminyname = csminy # min chip size\n",
    "        obj.csmaxxname = csmaxx; obj.csmaxyname = csmaxy # max chip size\n",
    "        obj.ssmname = ssm # stable surface mask\n",
    "        obj.winlocname = \"window_location.tif\"\n",
    "        obj.winoffname = \"window_offset.tif\"\n",
    "        obj.winsrname = \"window_search_range.tif\"\n",
    "        obj.wincsminname = \"window_chip_size_min.tif\"\n",
    "        obj.wincsmaxname = \"window_chip_size_max.tif\"\n",
    "        obj.winssmname = \"window_stable_surface_mask.tif\"\n",
    "        obj.winro2vxname = \"window_rdr_off2vel_x_vec.tif\"\n",
    "        obj.winro2vyname = \"window_rdr_off2vel_y_vec.tif\"\n",
    "\n",
    "        obj.runGeogrid() # RUN GEOGRID\n",
    "        print('Optical geogrid finished.'); print()\n",
    "\n",
    "    ############ Move files produced to the out_path directory ##############\n",
    "    for file in os.listdir(os.getcwd()):\n",
    "        if file.startswith('window') and file.endswith('.tif'):\n",
    "            shutil.move(os.getcwd()+'/'+file, out_path+file)\n",
    "    print('Geogrid output files moved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e20b7",
   "metadata": {},
   "source": [
    "## 3) Function to run autoRIFT with new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c8c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoRIFT  \n",
    "def run_autoRIFT_inhouse(indir_m, indir_s, out_path, img_type, mpflag, xGrid, yGrid, # required parameters\n",
    "                         FILTER, WALLISFILTERWIDTH, SPARSE_SEARCH_SAMPLE_RATIO, OVERSAMPLE_RATIO, MINCHIPSIZE,\n",
    "                         Dx0, Dy0, CSMINx0, SRx0, SRy0, CSMAXx0, CSMAXy0, SSM, # optional parameters\n",
    "                         noDataMask, nodataval, geogrid_run_info):\n",
    "    CHIPSIZE_M = MINCHIPSIZE # set minimum chip size equal\n",
    "    \n",
    "    # requires grid location from geogrid\n",
    "    origSize = xGrid.shape # grab original size from xGrid\n",
    "    \n",
    "    if img_type == 'OPT': ############# OPTICAL SETTINGS ############################# \n",
    "        print('Processing optical images with autoRIFT.'); print()\n",
    "        optflag = 1 # turn on optical flag\n",
    "        # Coregister and read in the two images (from loadProductOptical())\n",
    "        obj = GeogridOptical()\n",
    "        x1a, y1a, xsize1, ysize1, x2a, y2a, xsize2, ysize2, trans = obj.coregister(indir_m, indir_s, 0)\n",
    "\n",
    "        # read dates from filenames\n",
    "        indir_m = indir_m.split('/')[-1]; indir_s = indir_s.split('/')[-1]\n",
    "        if 'LC' in indir_m and 'LC' in indir_s:\n",
    "            ds1 = indir_m.split('_')[3]; ds2 = indir_s.split('_')[3]\n",
    "            sat = 'LS'\n",
    "        elif 'S2' in indir_m and 'S2' in indir_s:\n",
    "            ds1 = indir_m.split('_')[2]; ds2 = indir_s.split('_')[2]\n",
    "            sat = 'S2'\n",
    "        elif 'PS' in indir_m and 'PS' in indir_s:\n",
    "            ds1 = indir_m.split('_')[1]\n",
    "            ds2 = indir_s.split('_')[1]\n",
    "            sat = 'PS'\n",
    "        else:\n",
    "            raise Exception('Optical data NOT supported yet!')\n",
    "        \n",
    "            \n",
    "        print('DATES: ')\n",
    "        print(ds1, ds2); print(sat)\n",
    "\n",
    "        # read in the images\n",
    "        DS1 = gdal.Open(indir_m); DS2 = gdal.Open(indir_s)\n",
    "        I1 = DS1.ReadAsArray(xoff=x1a, yoff=y1a, xsize=xsize1, ysize=ysize1)\n",
    "        I1 = I1.astype(np.float32)\n",
    "        I2 = DS2.ReadAsArray(xoff=x2a, yoff=y2a, xsize=xsize2, ysize=ysize2)\n",
    "        I2 = I2.astype(np.float32)\n",
    "        DS1=None; DS2=None # clear DS1 and DS2\n",
    "\n",
    "        # Initialize autoRIFT object (from runAutorift())\n",
    "        obj = autoRIFT()\n",
    "#         obj.configure()\n",
    "\n",
    "    elif img_type == 'SAR': ############# SAR SETTINGS #############################  \n",
    "        print('Processing SAR images with autoRIFT.'); print()\n",
    "        optflag = 0 # turn off opt flag\n",
    "        # Read in the two SAR images (from loadProduct())\n",
    "        img1 = IML.mmapFromISCE(filename1, logging); I1 = IMG.bands[0]\n",
    "        img2 = IML.mmapFromISCE(filename2, logging); I2 = IMG.bands[0]\n",
    "        I1 = np.abs(I1); I2 = np.abs(I2) # SAR amplitude only\n",
    "        \n",
    "    else:\n",
    "        print(\"Image type not recognized. Use either 'OPT' or 'SAR'.\")\n",
    "        \n",
    "    ############# Initialize autoRIFT object (from runAutorift()) ##################\n",
    "    obj = autoRIFT()\n",
    "#     obj.configure()\n",
    "    \n",
    "    obj.MultiThread = mpflag # multiprocessing\n",
    "    obj.I1 = I1; obj.I2 = I2 # assign the images\n",
    "    obj.xGrid = xGrid; obj.yGrid = yGrid # assign the grid \n",
    "\n",
    "    # GENERATE NO DATA MASK\n",
    "    # where offset searching will be skipped based on \n",
    "    # 1) imported nodata mask and/or 2) zero values in the image\n",
    "    for ii in range(obj.xGrid.shape[0]):\n",
    "        for jj in range(obj.xGrid.shape[1]):\n",
    "            if (obj.yGrid[ii,jj] != nodata)&(obj.xGrid[ii,jj] != nodata):\n",
    "                if (I1[obj.yGrid[ii,jj]-1,obj.xGrid[ii,jj]-1]==0)|(I2[obj.yGrid[ii,jj]-1,obj.xGrid[ii,jj]-1]==0):\n",
    "                    noDataMask[ii,jj] = True\n",
    "                    \n",
    "    # SEARCH RANGE\n",
    "    if SRx0 is None:\n",
    "        # default is a zero array\n",
    "#        ###########     uncomment to customize SearchLimit based on velocity distribution \n",
    "        if Dx0 is not None:\n",
    "            obj.SearchLimitX = np.int32(4+(25-4)/(np.max(np.abs(Dx0[np.logical_not(noDataMask)]))-np.min(np.abs(Dx0[np.logical_not(noDataMask)])))*(np.abs(Dx0)-np.min(np.abs(Dx0[np.logical_not(noDataMask)]))))\n",
    "        else:\n",
    "            obj.SearchLimitX = 15\n",
    "        obj.SearchLimitY = 15\n",
    "#        ###########\n",
    "        obj.SearchLimitX = obj.SearchLimitX * np.logical_not(noDataMask)\n",
    "        obj.SearchLimitY = obj.SearchLimitY * np.logical_not(noDataMask)\n",
    "    else:\n",
    "        obj.SearchLimitX = SRx0\n",
    "        obj.SearchLimitY = SRy0\n",
    "       ############ add buffer to search range\n",
    "        obj.SearchLimitX[obj.SearchLimitX!=0] = obj.SearchLimitX[obj.SearchLimitX!=0] + 2\n",
    "        obj.SearchLimitY[obj.SearchLimitY!=0] = obj.SearchLimitY[obj.SearchLimitY!=0] + 2\n",
    "    \n",
    "    # CHIP SIZE\n",
    "    if CSMINx0 is not None:\n",
    "        obj.ChipSizeMaxX = CSMAXx0\n",
    "        obj.ChipSizeMinX = CSMINx0\n",
    "        \n",
    "        gridspacingx = MINCHIPSIZE # use the grid spacing from above\n",
    "        chipsizex0 = MINCHIPSIZE\n",
    "        pixsizex = trans[1] # grab from coregister function\n",
    "    \n",
    "        obj.ChipSize0X = int(np.ceil(chipsizex0/pixsizex/4)*4)\n",
    "        obj.GridSpacingX = int(obj.ChipSize0X*gridspacingx/chipsizex0)\n",
    "\n",
    "        RATIO_Y2X = CSMINy0/CSMINx0\n",
    "        obj.ScaleChipSizeY = np.median(RATIO_Y2X[(CSMINx0!=nodata)&(CSMINy0!=nodata)])\n",
    "#         obj.ScaleChipSizeY = 1 # USE SCALE OF 1 for square pixels\n",
    "    else:\n",
    "        if ((optflag == 1)&(xGrid is not None)):\n",
    "            obj.ChipSizeMaxX = 32 # pixels\n",
    "            obj.ChipSizeMinX = 16 # pixels\n",
    "            obj.ChipSize0X = 16 # pixels\n",
    "    \n",
    "    # DOWNSTREAM SEARCH OFFSET\n",
    "    if Dx0 is not None:\n",
    "        obj.Dx0 = Dx0\n",
    "        obj.Dy0 = Dy0\n",
    "    else:\n",
    "        obj.Dx0 = obj.Dx0 * np.logical_not(noDataMask)\n",
    "        obj.Dy0 = obj.Dy0 * np.logical_not(noDataMask)\n",
    "\n",
    "    # REPLACE NO DATA VALUES WITH 0\n",
    "    obj.xGrid[noDataMask] = 0\n",
    "    obj.yGrid[noDataMask] = 0\n",
    "    obj.Dx0[noDataMask] = 0\n",
    "    obj.Dy0[noDataMask] = 0\n",
    "    if SRx0 is not None:\n",
    "        obj.SearchLimitX[noDataMask] = 0\n",
    "        obj.SearchLimitY[noDataMask] = 0\n",
    "    if CSMINx0 is not None:\n",
    "        obj.ChipSizeMaxX[noDataMask] = 0\n",
    "        obj.ChipSizeMinX[noDataMask] = 0\n",
    "    \n",
    "    # convert azimuth offset to vertical offset as used in autoRIFT convention for SAR images\n",
    "    if optflag == 0:\n",
    "        obj.Dy0 = -1 * obj.Dy0\n",
    "        \n",
    "    ############## AutoRIFT Pre-processing (from runAutorift()) ############################\n",
    "    t1 = time.time()\n",
    "    print(\"Pre-process Start!!!\")\n",
    "    \n",
    "    # FILTERING:\n",
    "    if FILTER == 'WAL': \n",
    "        obj.preprocess_filt_wal() # WALLIS FILTER\n",
    "#         obj.zeroMask = 1 # removes edges\n",
    "        obj.WallisFilterWidth = WALLISFILTERWIDTH # optional, default supposedly 21\n",
    "    elif FILTER == 'HPS':\n",
    "        obj.preprocess_filt_hps() # HIGH PASS FILTER\n",
    "    elif FILTER == 'SOB':\n",
    "        obj.preprocess_filt_sob() # SOBEL FILTER\n",
    "    elif FILTER == 'LAP':\n",
    "        obj.preprocess_filt_lap()\n",
    "    elif FILTER == 'DB':\n",
    "        obj.preprocess_db() # LOGARITHMIC OPERATOR (NO FILTER), FOR TOPOGRAPHY\n",
    "    else:\n",
    "        print(FILTER, 'not recognized. Using default high pass filter instead.')\n",
    "        obj.preprocess_filt_hps() # HIGH PASS FILTER\n",
    "        \n",
    "    print(\"Pre-process Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    # CONVERT TO UNIFORM DATA TYPE\n",
    "    t1 = time.time()\n",
    "#    obj.DataType = 0\n",
    "    obj.uniform_data_type()\n",
    "    print(\"Uniform Data Type Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    # OTHER :\n",
    "    obj.sparseSearchSampleRate = 1\n",
    "#    obj.colfiltChunkSize = 4\n",
    "\n",
    "    obj.OverSampleRatio = 64\n",
    "    if CSMINx0 is not None:\n",
    "        obj.OverSampleRatio = {obj.ChipSize0X:16,obj.ChipSize0X*2:32,obj.ChipSize0X*4:64,obj.ChipSize0X*8:64}\n",
    "    \n",
    "    ####################### Run AutoRIFT (from runAutorift())  ############################\n",
    "    t1 = time.time()\n",
    "    print(\"AutoRIFT Start!!!\")\n",
    "    obj.runAutorift()\n",
    "    print(\"AutoRIFT Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    noDataMask = cv2.dilate(noDataMask.astype(np.uint8),kernel,iterations = 1)\n",
    "    noDataMask = noDataMask.astype(np.bool)\n",
    "\n",
    "    # AT THIS POINT, THESE VARIABLES WILL BE CREATED:\n",
    "    # obj.Dx, obj.Dy, obj.InterpMask, obj.ChipSizeX, obj.GridSpacingX, \n",
    "    # obj.ScaleChipSizeY, obj.SearchLimitX, obj.SearchLimitY, obj.origSize, noDataMask\n",
    "    \n",
    "    # PLOT RESULTS\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(15,5))\n",
    "    im1 = ax1.imshow(obj.Dx); ax1.set_title('Dx'); fig.colorbar(im1, ax=ax1)\n",
    "    im2 = ax2.imshow(obj.Dy); ax2.set_title('Dy'); fig.colorbar(im2, ax=ax2)\n",
    "    im3 = ax3.imshow(np.sqrt((obj.Dx**2) + (obj.Dy**2))); ax3.set_title('D_total'); fig.colorbar(im3,ax=ax3)\n",
    "    plt.suptitle(ds1+' to '+ds2)\n",
    "    plt.show()\n",
    "\n",
    "    ####################### Write outputs (from runAutorift())  ############################\n",
    "    t1 = time.time()\n",
    "    print(\"Write Outputs Start!!!\")\n",
    "          \n",
    "    # Write text file with parameters\n",
    "    f =  open(out_path+'parameters_'+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+'.txt', 'w')\n",
    "    f.write('Geogrid/AutoRIFT parameters for offset_'+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+'.tif:')\n",
    "    f.write('NO_DATA_VAL: '+str(NO_DATA_VAL))\n",
    "    f.write('Min chip size: '+str(MINCHIPSIZE))\n",
    "    f.write('DEM: '+dem)\n",
    "    f.write('dhdx: '+dhdx); f.write('dhdy: '+dhdy)\n",
    "    f.write('vx: '+vx); f.write('vy: '+vy)\n",
    "    f.write('srx: '+srx); f.write('sry: '+sry)\n",
    "    f.write('csminx: '+csminx); f.write('csminy: '+csminy)\n",
    "    f.write('csmaxx: '+csmaxx); f.write('csmaxy: '+csmaxy)\n",
    "    f.write('stable surface mask: '+ssm)\n",
    "    f.write('FILTER: '+FILTER)\n",
    "    f.write('WALLISFILTERWIDTH: '+str(WALLISFILTERWIDTH))\n",
    "    f.write('Spare search sample rate: '+str(SPARSE_SEARCH_SAMPLE_RATE))\n",
    "    f.write('Oversample ratio: '+str(OVERSAMPLE_RATIO))\n",
    "    if offset2vx is not None and offset2vy is not None:\n",
    "        f.write('Velocity.TIF file created.')\n",
    "    else:\n",
    "        f.write('Velocity.TIF not created.')\n",
    "    f.close() # close the parameter text file\n",
    "          \n",
    "    # open the window_location.tif file to gdalinfo\n",
    "    ds = gdal.Open(gp+'window_location.tif')\n",
    "    tran = ds.GetGeoTransform()\n",
    "    proj = ds.GetProjection()\n",
    "    srs = ds.GetSpatialRef()\n",
    "    \n",
    "    # initialize arrays\n",
    "    DX = np.zeros(origSize,dtype=np.float32) * np.nan; DY = np.zeros(origSize,dtype=np.float32) * np.nan\n",
    "    INTERPMASK = np.zeros(origSize,dtype=np.float32); CHIPSIZEX = np.zeros(origSize,dtype=np.float32)\n",
    "    SEARCHLIMITX = np.zeros(origSize,dtype=np.float32); SEARCHLIMITY = np.zeros(origSize,dtype=np.float32)\n",
    "    \n",
    "    # fill in arays\n",
    "    Dx = obj.Dx; Dy = obj.Dy; InterpMask = obj.InterpMask; ChipSizeX = obj.ChipSizeX\n",
    "    SearchLimitX = obj.SearchLimitX; SearchLimitY = obj.SearchLimitY\n",
    "    DX[0:Dx.shape[0],0:Dx.shape[1]] = Dx;  DY[0:Dy.shape[0],0:Dy.shape[1]] = Dy\n",
    "    INTERPMASK[0:InterpMask.shape[0],0:InterpMask.shape[1]] = InterpMask\n",
    "    CHIPSIZEX[0:ChipSizeX.shape[0],0:ChipSizeX.shape[1]] = ChipSizeX\n",
    "    SEARCHLIMITX[0:SearchLimitX.shape[0],0:SearchLimitX.shape[1]] = SearchLimitX\n",
    "    SEARCHLIMITY[0:SearchLimitY.shape[0],0:SearchLimitY.shape[1]] = SearchLimitY\n",
    "    \n",
    "    # mask out no data\n",
    "    DX[noDataMask] = np.nan; DY[noDataMask] = np.nan\n",
    "    INTERPMASK[noDataMask] = 0; CHIPSIZEX[noDataMask] = 0\n",
    "    SEARCHLIMITX[noDataMask] = 0; SEARCHLIMITY[noDataMask] = 0\n",
    "    if SSM is not None:\n",
    "        SSM[noDataMask] = False\n",
    "    DX[SEARCHLIMITX == 0] = np.nan; DY[SEARCHLIMITX == 0] = np.nan\n",
    "    INTERPMASK[SEARCHLIMITX == 0] = 0; CHIPSIZEX[SEARCHLIMITX == 0] = 0\n",
    "    if SSM is not None:\n",
    "        SSM[SEARCHLIMITX == 0] = False\n",
    "\n",
    "    # SAVE TO OFFSET.MAT FILE\n",
    "    sio.savemat('offset_'+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+'.mat', # offset mat filename\n",
    "                {'Dx':DX,'Dy':DY,'InterpMask':INTERPMASK,'ChipSizeX':CHIPSIZEX})\n",
    "    print('Offset.mat written.')\n",
    "    \n",
    "    # CREATE THE GEOTIFFS\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    # OFFSET.TIF\n",
    "    outRaster = driver.Create(\"offset_\"+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+\".tif\", # offset filename\n",
    "                              int(xGrid.shape[1]), int(xGrid.shape[0]), 5, gdal.GDT_Float32)\n",
    "    outRaster.SetGeoTransform(tran); outRaster.SetProjection(proj) # projections\n",
    "    outband = outRaster.GetRasterBand(1); outband.WriteArray(DX) # DX\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(2); outband.WriteArray(DY) # DY\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(3); outband.WriteArray(np.sqrt((DX**2) + (DY**2))) # DY\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(4); outband.WriteArray(INTERPMASK) # INTERPMASK\n",
    "    outband.FlushCache()\n",
    "    outband = outRaster.GetRasterBand(5); outband.WriteArray(CHIPSIZEX) # CHIPSIZE\n",
    "    outband.FlushCache()\n",
    "    del outRaster\n",
    "    print('Offset.tif written.')\n",
    "    \n",
    "    # VELOCITY.TIF\n",
    "    if offset2vx is not None and offset2vy is not None:\n",
    "        ds = gdal.Open(offset2vx) #### VX\n",
    "        band = ds.GetRasterBand(1); offset2vx_1 = band.ReadAsArray()\n",
    "        band = ds.GetRasterBand(2); offset2vx_2 = band.ReadAsArray()\n",
    "        if ds.RasterCount > 2:\n",
    "                band = ds.GetRasterBand(3)\n",
    "                offset2vr = band.ReadAsArray()\n",
    "        else:\n",
    "                offset2vr = None\n",
    "        band=None; ds=None\n",
    "        offset2vx_1[offset2vx_1 == nodata] = np.nan\n",
    "        offset2vx_2[offset2vx_2 == nodata] = np.nan\n",
    "\n",
    "        ds = gdal.Open(offset2vy) #### VY\n",
    "        band = ds.GetRasterBand(1); offset2vy_1 = band.ReadAsArray()\n",
    "        band = ds.GetRasterBand(2); offset2vy_2 = band.ReadAsArray()\n",
    "        if ds.RasterCount > 2:\n",
    "                band = ds.GetRasterBand(3)\n",
    "                offset2va = band.ReadAsArray()\n",
    "        else:\n",
    "                offset2va = None\n",
    "        band=None; ds=None\n",
    "        offset2vy_1[offset2vy_1 == nodata] = np.nan; offset2vy_2[offset2vy_2 == nodata] = np.nan\n",
    "        \n",
    "        if offset2va is not None:\n",
    "            offset2va[offset2va == nodata] = np.nan\n",
    "\n",
    "        VX = offset2vx_1 * DX + offset2vx_2 * DY\n",
    "        VY = offset2vy_1 * DX + offset2vy_2 * DY\n",
    "        VX = VX.astype(np.float32); VY = VY.astype(np.float32)\n",
    "\n",
    "        outRaster = driver.Create(\"velocity_\"+ds1+'_'+ds2+'_'+str(CHIPSIZE_M)+'m_'+sat+\".tif\", # velocity filename\n",
    "                                  int(xGrid.shape[1]), int(xGrid.shape[0]), 3, gdal.GDT_Float32)\n",
    "        outRaster.SetGeoTransform(tran); outRaster.SetProjection(proj)\n",
    "        outband = outRaster.GetRasterBand(1); outband.WriteArray(VX) # VX\n",
    "        outband.FlushCache()\n",
    "        outband = outRaster.GetRasterBand(2); outband.WriteArray(VY) # VY\n",
    "        outband.FlushCache()\n",
    "        outband = outRaster.GetRasterBand(3); outband.WriteArray(np.sqrt((VX**2) + (VY**2))) # V\n",
    "        outband.FlushCache()\n",
    "        del outRaster\n",
    "        print('Velocity.tif written.')\n",
    "    \n",
    "    print(\"Write Outputs Done!!!\")\n",
    "    print(time.time()-t1)\n",
    "    \n",
    "    # Move files produced to the out_path directory\n",
    "    for file in os.listdir(os.getcwd()):\n",
    "        if 'offset' in file or ('velocity' in file and '.tif' in file):\n",
    "            shutil.move(os.getcwd()+'/'+file, out_path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfe65c",
   "metadata": {},
   "source": [
    "# 4) Run geogrid and autoRIFT on all images in a folder\n",
    "\n",
    "Adjusted to run on all image pairs that adhere to the date separation in days (dt) and start/end date set below, for all chipsizes in the chipsize list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9025ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2path = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel2/' # input S2 images\n",
    "# LS8path = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/LS8images/useable_images/' # input LS8 images\n",
    "# PSpath = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/Planet_test/'\n",
    "# S1path = '/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/sentinel1/'\n",
    "# PSpath = '/Volumes/SGlacier/TG_20_21/'\n",
    "# boxpath = '/Users/surging/Documents/TG/BoxTurner/BoxTurner_UTM_07.shp' # the shapefile for Turner\n",
    "# autoriftpath = '/Users/surging/Documents/TG/autoRIFT/' # path to the autorift scripts\n",
    "# vmap_path = '/Users/surging/Documents/TG/vmap_test/' # output velocity map folder\n",
    "# basepath = '/Users/surging/Documents/TG/optical-offset-tracking/' # path where this script is located\n",
    "LS8path = '/Volumes/LaCie/Variegated_LS_images/' # input LS8 images\n",
    "\n",
    "######### Set minimum and maximum time separation and the platform (L8, S2, PS, S1) ###############\n",
    "platform = 'L8'\n",
    "startdate = '20120101' # inclusive start date\n",
    "enddate = '20220101' # inclusive end date\n",
    "min_dt = 5 # minimum time separation between images\n",
    "max_dt = 60 # maximum time separation between images\n",
    "###############################################################################################\n",
    "\n",
    "chipsize_list = [100,200,300] # meters\n",
    "out_path = '/Users/jukesliu/Documents/TURNER/DATA/VELOCITY_MAPS/output_AutoRIFT/' # output file path\n",
    "NO_DATA_VAL = -32767 # no data value in the output products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d162db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LC08_L1TP_062018_20131007_20200913_02_T1_B8_Bu...</td>\n",
       "      <td>20131007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>LC08_L1TP_061018_20221025_20221107_02_T1_B8_Bu...</td>\n",
       "      <td>20221025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>LC08_L1TP_062018_20221117_20221128_02_T2_B8_Bu...</td>\n",
       "      <td>20221117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>LC08_L1TP_062018_20221117_20221128_02_T2_B8_Bu...</td>\n",
       "      <td>20221117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>LC08_L1TP_062018_20221203_20221212_02_T1_B8_Bu...</td>\n",
       "      <td>20221203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>LC08_L1TP_062018_20221203_20221212_02_T1_B8_Bu...</td>\n",
       "      <td>20221203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename      date\n",
       "0    LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...  20130610\n",
       "1    LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...  20130610\n",
       "2    LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...  20130712\n",
       "3    LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...  20130712\n",
       "4    LC08_L1TP_062018_20131007_20200913_02_T1_B8_Bu...  20131007\n",
       "..                                                 ...       ...\n",
       "161  LC08_L1TP_061018_20221025_20221107_02_T1_B8_Bu...  20221025\n",
       "162  LC08_L1TP_062018_20221117_20221128_02_T2_B8_Bu...  20221117\n",
       "163  LC08_L1TP_062018_20221117_20221128_02_T2_B8_Bu...  20221117\n",
       "164  LC08_L1TP_062018_20221203_20221212_02_T1_B8_Bu...  20221203\n",
       "165  LC08_L1TP_062018_20221203_20221212_02_T1_B8_Bu...  20221203\n",
       "\n",
       "[166 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the folder path based on the platform\n",
    "if platform == 'S2': # sentinel-2\n",
    "    path = s2path\n",
    "    ext = '_clipped.tif' # image filename extension\n",
    "    img_type = 'OPT'\n",
    "    date_split_idx = 2 # split filename by underscore, index corresponds to image date\n",
    "elif platform == 'L8': # landsat 8\n",
    "    path = LS8path\n",
    "    ext = '.TIF'\n",
    "    img_type = 'OPT'\n",
    "    date_split_idx = 3\n",
    "elif platform == 'PS': # PlanetScope\n",
    "    path = PSpath\n",
    "    ext = '_clipped.tif'  \n",
    "    img_type = 'OPT'\n",
    "    date_split_idx = 1\n",
    "else:\n",
    "    print('Platform', platform, 'not recognized. Options are \"S2\", \"L8\", and \"PS\"')\n",
    "\n",
    "# record all possible images and their dates\n",
    "dates = []; files = []  \n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(ext):\n",
    "        date = file.split('_')[date_split_idx] # grab the date from the filename\n",
    "        dates.append(date); files.append(file) # store the date and filename\n",
    "files_df = pd.DataFrame(list(zip(files,dates)),columns=['filename','date'])\n",
    "files_df = files_df.sort_values(by='date',ignore_index=True) # sort the dataframe by ascending date\n",
    "files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f33200f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LC08_L1TP_062018_20131007_20200913_02_T1_B8_Bu...</td>\n",
       "      <td>20131007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>LC08_L1TP_062018_20201010_20201016_02_T1_B8_Bu...</td>\n",
       "      <td>20201010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>LC08_L1TP_061018_20201019_20201105_02_T1_B8_Bu...</td>\n",
       "      <td>20201019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>LC08_L1TP_061018_20201019_20201105_02_T1_B8_Bu...</td>\n",
       "      <td>20201019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>LC08_L1TP_061018_20201120_20210314_02_T1_B8_Bu...</td>\n",
       "      <td>20201120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>LC08_L1TP_061018_20201120_20210314_02_T1_B8_Bu...</td>\n",
       "      <td>20201120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename      date\n",
       "0    LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...  20130610\n",
       "1    LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...  20130610\n",
       "2    LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...  20130712\n",
       "3    LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...  20130712\n",
       "4    LC08_L1TP_062018_20131007_20200913_02_T1_B8_Bu...  20131007\n",
       "..                                                 ...       ...\n",
       "141  LC08_L1TP_062018_20201010_20201016_02_T1_B8_Bu...  20201010\n",
       "142  LC08_L1TP_061018_20201019_20201105_02_T1_B8_Bu...  20201019\n",
       "143  LC08_L1TP_061018_20201019_20201105_02_T1_B8_Bu...  20201019\n",
       "144  LC08_L1TP_061018_20201120_20210314_02_T1_B8_Bu...  20201120\n",
       "145  LC08_L1TP_061018_20201120_20210314_02_T1_B8_Bu...  20201120\n",
       "\n",
       "[146 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out those before and afte the input start and end date\n",
    "files_df = files_df[(files_df.date >= startdate) & (files_df.date <= enddate)]\n",
    "files_df = files_df.reset_index(drop=True) # reset index for searching\n",
    "files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97e55818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...</td>\n",
       "      <td>20130712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LC08_L1TP_062018_20131007_20200913_02_T1_B8_Bu...</td>\n",
       "      <td>20131007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>LC08_L1TP_062018_20201010_20201016_02_T1_B8_Bu...</td>\n",
       "      <td>20201010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>LC08_L1TP_061018_20201019_20201105_02_T1_B8_Bu...</td>\n",
       "      <td>20201019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>LC08_L1TP_061018_20201019_20201105_02_T1_B8_Bu...</td>\n",
       "      <td>20201019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>LC08_L1TP_061018_20201120_20210314_02_T1_B8_Bu...</td>\n",
       "      <td>20201120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>LC08_L1TP_061018_20201120_20210314_02_T1_B8_Bu...</td>\n",
       "      <td>20201120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename      date\n",
       "0    LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...  20130610\n",
       "1    LC08_L1TP_061018_20130610_20200912_02_T1_B8_Bu...  20130610\n",
       "2    LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...  20130712\n",
       "3    LC08_L1TP_061018_20130712_20200912_02_T1_B8_Bu...  20130712\n",
       "4    LC08_L1TP_062018_20131007_20200913_02_T1_B8_Bu...  20131007\n",
       "..                                                 ...       ...\n",
       "141  LC08_L1TP_062018_20201010_20201016_02_T1_B8_Bu...  20201010\n",
       "142  LC08_L1TP_061018_20201019_20201105_02_T1_B8_Bu...  20201019\n",
       "143  LC08_L1TP_061018_20201019_20201105_02_T1_B8_Bu...  20201019\n",
       "144  LC08_L1TP_061018_20201120_20210314_02_T1_B8_Bu...  20201120\n",
       "145  LC08_L1TP_061018_20201120_20210314_02_T1_B8_Bu...  20201120\n",
       "\n",
       "[146 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicate dates for S2, keep the original over the PS imitation\n",
    "if platform == 'S2':\n",
    "    unique_dates = list(set(files_df.date))\n",
    "    for date in unique_dates:\n",
    "        date_df = files_df[files_df.date == date]\n",
    "        if len(date_df) > 1: # if there's a duplicate\n",
    "            for idx, row in date_df.iterrows():\n",
    "                if row.filename.startswith('S2P'):\n",
    "                    files_df = files_df.drop(idx)\n",
    "files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c176eb",
   "metadata": {},
   "source": [
    "## Once satisfied with the image list, adjust the code below to indicate the satellite platforms used (S2, L8, PS):\n",
    "\n",
    "Image naming convention:\n",
    "\n",
    "Sentinel-2 (S2) images contain the date as the third item split by underscores \"_\" and end in \"_clipped.tif\".\n",
    "\n",
    "Landsat 8 (L8) images contain the date as the fourth item split by underscores \"_\" and end in \"TIF\".\n",
    "\n",
    "PlanetScope (PS) images contain the date as the second item split by underscores \"_\" and end in \"_clipped.tif\".\n",
    "\n",
    "#### If your image naming convention differs, adjust L6-28 in the cell below.\n",
    "\n",
    "## Also set the optional inputs within the cell (L56-78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f4d35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try multiple chip sizes and sats\n",
    "# for platform in ['S2','L8']:\n",
    "for platform in ['L8']:\n",
    "# for platform in [platforms]:\n",
    "    # assign the folder path based on the platform\n",
    "    if platform == 'S2': # sentinel-2\n",
    "        path = s2path\n",
    "        ext = '_clipped.tif' # image filename extension\n",
    "        img_type = 'OPT'\n",
    "        date_split_idx = 2 # split filename by underscore, index corresponds to image date\n",
    "    elif platform == 'L8': # landsat 8\n",
    "        path = LS8path\n",
    "        ext = '.TIF'\n",
    "        img_type = 'OPT'\n",
    "        date_split_idx = 3\n",
    "    elif platform == 'PS': # PlanetScope\n",
    "        path = PSpath\n",
    "        ext = '_clipped.tif'  \n",
    "        img_type = 'OPT'\n",
    "        date_split_idx = 1\n",
    "    else:\n",
    "        print('Platform', platform, 'not recognized. Options are \"S2\", \"L8\", and \"PS\"')\n",
    "\n",
    "    # record all possible images and their dates\n",
    "    dates = []; files = []  \n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(ext):\n",
    "            date = file.split('_')[date_split_idx] # grab the date from the filename\n",
    "            dates.append(date); files.append(file) # store the date and filename\n",
    "    files_df = pd.DataFrame(list(zip(files,dates)),columns=['filename','date'])\n",
    "    files_df = files_df.sort_values(by='date',ignore_index=True) # sort the dataframe by ascending date\n",
    "    files_df = files_df[(files_df.date >= startdate) & (files_df.date <= enddate)]\n",
    "    files_df = files_df.reset_index(drop=True) # reset index for searching\n",
    "    print(\"Number of files found:\",len(files_df))\n",
    "    \n",
    "    for min_chipsize in chipsize_list:\n",
    "        # automatically set file names\n",
    "        dhdx_outfile = demname[:-4]+'_'+str(CHIPSIZE_M)+'m_dhdx.tif' # generate new filename\n",
    "        dhdy_outfile = demname[:-4]+'_'+str(CHIPSIZE_M)+'m_dhdy.tif' # generate new filename\n",
    "        ssm_outfile = 'ssm_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "        vx_outfile = 'vx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "        vy_outfile = 'vy_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "        srx_outfile = 'srx_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "        sry_outfile = 'sry_'+str(CHIPSIZE_M)+'m.tif' # generate new filename\n",
    "        csminx_fname = 'csminx_'+str(CHIPSIZE_M)+'m.tif'\n",
    "        csminy_fname = 'csminy_'+str(CHIPSIZE_M)+'m.tif'\n",
    "        csmaxx_fname = 'csmaxx_'+str(CHIPSIZE_M)+'m.tif'\n",
    "        csmaxy_fname = 'csmaxy_'+str(CHIPSIZE_M)+'m.tif'\n",
    "        \n",
    "        MINCHIPSIZE = min_chipsize\n",
    "        # generate geogrid inputs and grab outfilenames\n",
    "        [dem_outfile, dhdx_outfile, dhdy_outfile, \n",
    "         vx_outfile, vy_outfile, srx_outfile, \n",
    "         sry_outfile, ssm_outfile] = generate_geogrid_inputs(min_chipsize, dempath, demname, \n",
    "                                                                         refvpath, vx_fname, vy_fname, \n",
    "                                                                         sr_scaling)\n",
    "#         [dem_outfile, dhdx_outfile, dhdy_outfile] = generate_geogrid_inputs(min_chipsize, dempath, demname, \n",
    "#                                                                          refvpath, vx_fname, vy_fname, \n",
    "#                                                                          sr_scaling)\n",
    "        #######################################################################\n",
    "        # # CHOOSE OPTIONAL INPUTS: (set as '' to leave blank)\n",
    "        # # 1) surface slope:\n",
    "        #   dhdx = ''; dhdy = ''  \n",
    "        dhdx = dempath+dhdx_outfile; dhdy = dempath+dhdy_outfile\n",
    "        \n",
    "        # # 2) reference velocity:\n",
    "#         vx = ''; vy = ''    \n",
    "        vx = refvpath+vx_fname; vy = refvpath+vy_fname\n",
    "        \n",
    "        # # 3) chip sizes:\n",
    "        csminx = ''; csminy = ''\n",
    "        csmaxx = ''; csmaxy = ''  \n",
    "        #             csminx = refvpath+csminx_fname; csminy = refvpath+csminy_fname\n",
    "        #             csmaxx = refvpath+csmaxx_fname; csmaxy = refvpath+csmaxy_fname\n",
    "        \n",
    "        # # 4) stable surface mask:\n",
    "        ssm = ''\n",
    "        #             ssm = refvpath+ssm_outfile # stable surface mask \n",
    "        \n",
    "        # # 5) search range limit:\n",
    "        srx = ''; sry = '' # for best results, never input search range limit\n",
    "        #######################################################################\n",
    "        \n",
    "        for rownum in range(0,len(files_df)-1):\n",
    "            if rownum == 0: # for the first row, idx1 = 0 and idx2 = 1\n",
    "                idx1 = rownum\n",
    "#             idx2 = idx1+1 # reset index 2 as the next item - SEQUENTIAL\n",
    "            for idx2, row in files_df[idx1:].iterrows(): # for all subsequent idxs - ALL PAIRS\n",
    "\n",
    "                if idx1 < len(files_df) and idx2 < len(files_df): # don't surpass the end of the data\n",
    "                    # identify the successive image pairs:\n",
    "                    m = files_df.loc[idx1,'filename']; s = files_df.loc[idx2, 'filename']\n",
    "\n",
    "                    # grab the two dates and convert to datetime objects\n",
    "                    d1s = m.split('_')[date_split_idx]; d2s = s.split('_')[date_split_idx]\n",
    "                    if platform == 'S1': # S1 filenames need another split\n",
    "                        d1s = d1s[:8]; d2s = d2s[:8]\n",
    "\n",
    "                    # calculate time separation\n",
    "                    d1 = datetime.datetime.strptime(d1s, '%Y%m%d'); d2 = datetime.datetime.strptime(d2s, '%Y%m%d')\n",
    "                    dt = d2-d1; dt = int(dt.days)\n",
    "                    \n",
    "\n",
    "                    # run geogrid and autoRIFT\n",
    "                    if m is not None and s is not None:\n",
    "                        print(m,s)\n",
    "                        indir_m = path+m; indir_s = path+s # path to the two images\n",
    "\n",
    "                        dem = dempath+demname\n",
    "                        dhdx = dempath+dhdx_outfile\n",
    "                        dhdy = dempath+dhdy_outfile\n",
    "                        ###################### RUN GEOGRID ################################\n",
    "                        run_geogrid_inhouse(out_path, img_type, indir_m, indir_s, MINCHIPSIZE, NO_DATA_VAL, dem, # required inputs\n",
    "                                            dhdx, dhdy, vx, vy, srx, sry, csminx, csminy, csmaxx, csmaxy, ssm, # optional inputs\n",
    "                                            )\n",
    "\n",
    "                        ##################### PREP AUTORIFT ##############################\n",
    "                        gp = out_path # identify files produced from geogrid\n",
    "                        # remove all empty grids\n",
    "                        for grid in os.listdir(gp): \n",
    "                            if grid.startswith('window') and grid.endswith('.tif'):\n",
    "                                reader = rio.open(gp+grid) # read dataset\n",
    "                                data_found = False \n",
    "                                for band in range(1,reader.count+1):\n",
    "                                    testband = reader.read(band) # read in the band\n",
    "                                    if np.count_nonzero(testband[testband != NO_DATA_VAL]) > 0:\n",
    "                                        data_found = True\n",
    "                                if not data_found:\n",
    "                                    print(grid, 'has no data. Removed.')\n",
    "                                    os.remove(gp+grid)\n",
    "\n",
    "                        # fill in AutoRIFT parameters using the files\n",
    "                        mpflag = 0 # leave multiprocessing off\n",
    "\n",
    "                        # GRID LOCATION (required) from window_location.tif\n",
    "                        grid_location = rio.open(gp+'window_location.tif')\n",
    "                        xGrid = grid_location.read(1) # 1st band in window location\n",
    "                        yGrid = grid_location.read(2) # 2nd band in window location\n",
    "\n",
    "                        # optional parameters (default None or zero until filled)\n",
    "                        init_offset = None; search_range = None\n",
    "                        chip_size_min = None; chip_size_max = None\n",
    "                        offset2vx = None; offset2vy = None; stable_surface_mask = None\n",
    "                        Dx0 = None; Dy0 = None; CSMINx0 = None\n",
    "                        SRx0 = None; SRy0 = None;\n",
    "                        CSMAXx0 = None; CSMAXy0 = None; SSM = None\n",
    "                        noDataMask = np.zeros(xGrid.shape).astype(int)\n",
    "\n",
    "                        if os.path.exists(gp+'window_offset.tif'): # Dx0 and Dy0 from window_offset.tif\n",
    "                            init_offset = rio.open(gp+'window_offset.tif')\n",
    "                            Dx0 = init_offset.read(1); Dy0 = init_offset.read(2)\n",
    "                        if os.path.exists(gp+'window_search_range.tif'): # SRx0 and SRy0 from window_search_range.tif\n",
    "                            search_range = rio.open(gp+'window_search_range.tif')\n",
    "                            SRx0 = search_range.read(1); SRy0 = search_range.read(2)\n",
    "                        if os.path.exists(gp+'window_chip_size_min.tif'): # CSMINx0 and CSMINy0 from window_chip_size_min.tif\n",
    "                            chip_size_min = rio.open(gp+'window_chip_size_min.tif')\n",
    "                            CSMINx0 = chip_size_min.read(1); CSMINy0 = chip_size_min.read(2)\n",
    "                        if os.path.exists(gp+'window_chip_size_max.tif'): # CSMAXx0 and CSMAXy0 from window_chip_size_max.tif\n",
    "                            chip_size_max = rio.open(gp+'window_chip_size_max.tif')\n",
    "                            CSMAXx0 = chip_size_max.read(1); CSMAXy0 = chip_size_max.read(2)\n",
    "                        if os.path.exists(gp+'window_rdr_off2vel_x_vec.tif'): # offset2vx from window_rdr_off2vel_x_vec.tif\n",
    "                            offset2vx = gp+'window_rdr_off2vel_x_vec.tif' # path to be read in with GDAL\n",
    "                        if os.path.exists(gp+'window_rdr_off2vel_y_vec.tif'): # offset2vy from window_rdr_off2vel_y_vec.tif\n",
    "                            offset2vy = gp+'window_rdr_off2vel_y_vec.tif' \n",
    "                        if os.path.exists(gp+'window_stable_surface_mask.tif'): # noDataMask from window_stable_surface_mask.tif\n",
    "                            stable_surface_mask = rio.open(gp+'window_stable_surface_mask.tif')\n",
    "                            noDataMask = stable_surface_mask.read(1)\n",
    "\n",
    "                        # other parameters\n",
    "                        nodata = NO_DATA_VAL # use same as in previous steps\n",
    "                        geogrid_run_info=None\n",
    "                        print('AutoRIFT parameters loaded.')\n",
    "\n",
    "                    ##################### RUN AUTORIFT ##############################\n",
    "                    print(indir_m, indir_s)\n",
    "                    # run autoRIFT with function\n",
    "                    run_autoRIFT_inhouse(indir_m, indir_s, out_path, img_type, mpflag, xGrid, yGrid, # required parameters\n",
    "                                             FILTER, WALLISFILTERWIDTH, SPARSE_SEARCH_SAMPLE_RATE, \n",
    "                                             OVERSAMPLE_RATIO, MINCHIPSIZE,\n",
    "                                             Dx0, Dy0, CSMINx0, SRx0, SRy0, CSMAXx0, CSMAXy0, SSM, # optional parameters\n",
    "                                         noDataMask, nodata, geogrid_run_info=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f6b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newautoriftenv",
   "language": "python",
   "name": "newautoriftenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
